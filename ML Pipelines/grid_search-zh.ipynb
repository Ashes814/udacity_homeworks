{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 网格搜索\n",
    "现在，在你的建模的过程中引入网格搜索。首先，在下方加入一条 import 语句导入 `GridSearchCV`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/oo/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/oo/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/oo/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(['punkt', 'wordnet', 'averaged_perceptron_tagger'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_regex = 'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n",
    "\n",
    "def tokenize(text):\n",
    "    detected_urls = re.findall(url_regex, text)\n",
    "    for url in detected_urls:\n",
    "        text = text.replace(url, \"urlplaceholder\")\n",
    "\n",
    "    tokens = word_tokenize(text)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    clean_tokens = []\n",
    "    for tok in tokens:\n",
    "        clean_tok = lemmatizer.lemmatize(tok).lower().strip()\n",
    "        clean_tokens.append(clean_tok)\n",
    "\n",
    "    return clean_tokens\n",
    "\n",
    "\n",
    "class StartingVerbExtractor(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def starting_verb(self, text):\n",
    "        sentence_list = nltk.sent_tokenize(text)\n",
    "        for sentence in sentence_list:\n",
    "            pos_tags = nltk.pos_tag(tokenize(sentence))\n",
    "            first_word, first_tag = pos_tags[0]\n",
    "            if first_tag in ['VB', 'VBP'] or first_word == 'RT':\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_tagged = pd.Series(X).apply(self.starting_verb)\n",
    "        return pd.DataFrame(X_tagged)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 查看参数\n",
    "在修改 build_model 方法以加入网格搜索之前，先看看这里管道参数的设置。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('features', FeatureUnion([\n",
    "\n",
    "        ('text_pipeline', Pipeline([\n",
    "            ('vect', CountVectorizer(tokenizer=tokenize)),\n",
    "            ('tfidf', TfidfTransformer())\n",
    "        ])),\n",
    "\n",
    "        ('starting_verb', StartingVerbExtractor())\n",
    "    ])),\n",
    "\n",
    "    ('clf', RandomForestClassifier())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'memory': None,\n",
       " 'steps': [('features',\n",
       "   FeatureUnion(transformer_list=[('text_pipeline',\n",
       "                                   Pipeline(steps=[('vect',\n",
       "                                                    CountVectorizer(tokenizer=<function tokenize at 0x7fe71b14caf0>)),\n",
       "                                                   ('tfidf',\n",
       "                                                    TfidfTransformer())])),\n",
       "                                  ('starting_verb', StartingVerbExtractor())])),\n",
       "  ('clf', RandomForestClassifier())],\n",
       " 'verbose': False,\n",
       " 'features': FeatureUnion(transformer_list=[('text_pipeline',\n",
       "                                 Pipeline(steps=[('vect',\n",
       "                                                  CountVectorizer(tokenizer=<function tokenize at 0x7fe71b14caf0>)),\n",
       "                                                 ('tfidf',\n",
       "                                                  TfidfTransformer())])),\n",
       "                                ('starting_verb', StartingVerbExtractor())]),\n",
       " 'clf': RandomForestClassifier(),\n",
       " 'features__n_jobs': None,\n",
       " 'features__transformer_list': [('text_pipeline',\n",
       "   Pipeline(steps=[('vect',\n",
       "                    CountVectorizer(tokenizer=<function tokenize at 0x7fe71b14caf0>)),\n",
       "                   ('tfidf', TfidfTransformer())])),\n",
       "  ('starting_verb', StartingVerbExtractor())],\n",
       " 'features__transformer_weights': None,\n",
       " 'features__verbose': False,\n",
       " 'features__text_pipeline': Pipeline(steps=[('vect',\n",
       "                  CountVectorizer(tokenizer=<function tokenize at 0x7fe71b14caf0>)),\n",
       "                 ('tfidf', TfidfTransformer())]),\n",
       " 'features__starting_verb': StartingVerbExtractor(),\n",
       " 'features__text_pipeline__memory': None,\n",
       " 'features__text_pipeline__steps': [('vect',\n",
       "   CountVectorizer(tokenizer=<function tokenize at 0x7fe71b14caf0>)),\n",
       "  ('tfidf', TfidfTransformer())],\n",
       " 'features__text_pipeline__verbose': False,\n",
       " 'features__text_pipeline__vect': CountVectorizer(tokenizer=<function tokenize at 0x7fe71b14caf0>),\n",
       " 'features__text_pipeline__tfidf': TfidfTransformer(),\n",
       " 'features__text_pipeline__vect__analyzer': 'word',\n",
       " 'features__text_pipeline__vect__binary': False,\n",
       " 'features__text_pipeline__vect__decode_error': 'strict',\n",
       " 'features__text_pipeline__vect__dtype': numpy.int64,\n",
       " 'features__text_pipeline__vect__encoding': 'utf-8',\n",
       " 'features__text_pipeline__vect__input': 'content',\n",
       " 'features__text_pipeline__vect__lowercase': True,\n",
       " 'features__text_pipeline__vect__max_df': 1.0,\n",
       " 'features__text_pipeline__vect__max_features': None,\n",
       " 'features__text_pipeline__vect__min_df': 1,\n",
       " 'features__text_pipeline__vect__ngram_range': (1, 1),\n",
       " 'features__text_pipeline__vect__preprocessor': None,\n",
       " 'features__text_pipeline__vect__stop_words': None,\n",
       " 'features__text_pipeline__vect__strip_accents': None,\n",
       " 'features__text_pipeline__vect__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       " 'features__text_pipeline__vect__tokenizer': <function __main__.tokenize(text)>,\n",
       " 'features__text_pipeline__vect__vocabulary': None,\n",
       " 'features__text_pipeline__tfidf__norm': 'l2',\n",
       " 'features__text_pipeline__tfidf__smooth_idf': True,\n",
       " 'features__text_pipeline__tfidf__sublinear_tf': False,\n",
       " 'features__text_pipeline__tfidf__use_idf': True,\n",
       " 'clf__bootstrap': True,\n",
       " 'clf__ccp_alpha': 0.0,\n",
       " 'clf__class_weight': None,\n",
       " 'clf__criterion': 'gini',\n",
       " 'clf__max_depth': None,\n",
       " 'clf__max_features': 'auto',\n",
       " 'clf__max_leaf_nodes': None,\n",
       " 'clf__max_samples': None,\n",
       " 'clf__min_impurity_decrease': 0.0,\n",
       " 'clf__min_samples_leaf': 1,\n",
       " 'clf__min_samples_split': 2,\n",
       " 'clf__min_weight_fraction_leaf': 0.0,\n",
       " 'clf__n_estimators': 100,\n",
       " 'clf__n_jobs': None,\n",
       " 'clf__oob_score': False,\n",
       " 'clf__random_state': None,\n",
       " 'clf__verbose': 0,\n",
       " 'clf__warm_start': False}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 修改 `build_model` 函数，返回 GridSearchCV 对象\n",
    "试着用网格搜索尝试一些数据转换步骤和分类器的参数。浏览上面那些你可以搜索的参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "def build_model():\n",
    "    pipeline = Pipeline([\n",
    "        ('features', FeatureUnion([\n",
    "            \n",
    "            ('text_pipeline', Pipeline([\n",
    "                ('vect', CountVectorizer(tokenizer=tokenize)),\n",
    "                ('tfidf', TfidfTransformer())\n",
    "            ])),\n",
    "\n",
    "            ('starting_verb', StartingVerbExtractor())\n",
    "        ])),\n",
    "    \n",
    "        ('clf', RandomForestClassifier())\n",
    "    ])\n",
    "\n",
    "    # specify parameters for grid search\n",
    "    parameters = {\n",
    "        'clf__n_estimators': [100, 200, 300],\n",
    "        'clf__max_depth': [10, 20]\n",
    "\n",
    "    }\n",
    "\n",
    "    # create grid search object\n",
    "    cv = GridSearchCV(pipeline, param_grid=parameters)\n",
    "    \n",
    "    return cv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 运行测试\n",
    "运行网格搜索会花费你一些时间，尤其是要搜索很多参数的时候。如果你想要把时间控制在几分钟，可以试着注释掉一些要网格搜索的参数，仅保留1-2个参数，每个参数只尝试几个值。如果你发现该方法有效，再尝试加入更多参数，看看你的模型最终表现有多好。你可以在下一页进行尝试。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels: ['Action' 'Dialogue' 'Information']\n",
      "Confusion Matrix:\n",
      " [[ 57   0  52]\n",
      " [  1  15  25]\n",
      " [  0   0 451]]\n",
      "Accuracy: 0.870216306156406\n",
      "\n",
      "Best Parameters: {'clf__max_depth': 20, 'clf__n_estimators': 200}\n"
     ]
    }
   ],
   "source": [
    "def load_data():\n",
    "    df = pd.read_csv('corporate_messaging.csv', encoding='latin-1')\n",
    "    df = df[(df[\"category:confidence\"] == 1) & (df['category'] != 'Exclude')]\n",
    "    X = df.text.values\n",
    "    y = df.category.values\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def display_results(cv, y_test, y_pred):\n",
    "    labels = np.unique(y_pred)\n",
    "    confusion_mat = confusion_matrix(y_test, y_pred, labels=labels)\n",
    "    accuracy = (y_pred == y_test).mean()\n",
    "\n",
    "    print(\"Labels:\", labels)\n",
    "    print(\"Confusion Matrix:\\n\", confusion_mat)\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    print(\"\\nBest Parameters:\", cv.best_params_)\n",
    "\n",
    "\n",
    "def main():\n",
    "    X, y = load_data()\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "    model = build_model()\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    display_results(model, y_test, y_pred)\n",
    "\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
